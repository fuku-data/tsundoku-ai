import ast
import os

import openai
import settings.language
import settings.llm
import streamlit as st
from scripts.search_index import search_index


def main():
    settings.language.session_init_render()
    settings.llm.session_init_render()
    # NOTE: é–‹ç™ºã—ã‚„ã™ã„ã‚ˆã†ã«APIã‚­ãƒ¼ã‚’GUIã‹ã‚‰è¨­å®šã™ã‚‹æ‰‹é–“ã‚’çœã
    key: str = os.getenv("OPENAI_API_KEY")
    if len(key) > 0:
        st.session_state.llm_openai_api_key = key
        st.session_state.llm_connection_flag = True
        st.session_state.llm_click_set_or_clear_button_result = "success"
        # st.session_state.llm_set_or_clear_button_disabled = not st.session_state.llm_set_or_clear_button_disabled
    llm_assistant_ui()
    model_name, temperature = select_model()
    init_messages()
    converse_with_ai(model_name, temperature)


def llm_assistant_ui():
    st.set_page_config(
        page_title="ç©èª­PDFæ•‘æ¸ˆã‚¢ãƒ—ãƒª",
        page_icon="ğŸ“˜",
        layout="wide",
        initial_sidebar_state="expanded",
    )
    st.header("ğŸ“˜ ç©èª­PDFæ•‘æ¸ˆã‚¢ãƒ—ãƒª")
    st.header(f"ğŸ¤– {st.session_state.lg_AI_Assistant}")
    with st.empty():
        if not st.session_state.llm_connection_flag:
            st.warning(
                f"""
                ##### **ğŸ”” {st.session_state.lg_No_OpenAI_API_key_is_provided}**
                """
            )


def select_model():
    model = st.sidebar.radio(
        f"{st.session_state.lg_Choose_Model} :", ["GPT-3.5 Turbo", "GPT-4"]
    )
    if model == "GPT-3.5 Turbo":
        model_name = "gpt-3.5-turbo"
    else:
        model_name = "gpt-4"

    temperature = st.sidebar.slider(
        f"{st.session_state.lg_Temperature} :",
        min_value=0.0,
        max_value=2.0,
        value=0.0,
        step=0.1,
    )

    return model_name, temperature


def init_messages():
    clear_button = st.sidebar.button("Clear Chats", key="clear")
    if clear_button or "messages" not in st.session_state:
        system_prompt = """
            ## å›ç­”ãƒ«ãƒ¼ãƒ«
            - ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•æ–‡ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’åˆ†æã—ã€listå‹ã§è¿”å´ã—ã¦ãã ã•ã„ã€‚
            - ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå˜ä¸€ã®å ´åˆã¯ã€1è¦ç´ ã®listã€è¤‡æ•°ã®å ´åˆã¯ãã®å€‹æ•°åˆ†ã®è¦ç´ ã‚’æŒã£ãŸlistã§è¿”å´ã—ã¦ãã ã•ã„ã€‚

            ## è³ªå•æ–‡ã¨ãã‚Œã«å¯¾ã™ã‚‹è¿”å´å€¤ã®ä¾‹
            ###ä¾‹1
            - è³ªå•ï¼š`ãƒ‡ãƒ¼ã‚¿ã‚¬ãƒãƒŠãƒ³ã‚¹ã«ã¤ã„ã¦æ•™ãˆã¦ä¸‹ã•ã„ã€‚`
            - è¿”å´å€¤ï¼š`['ãƒ‡ãƒ¼ã‚¿ã‚¬ãƒãƒŠãƒ³ã‚¹']`
            ###ä¾‹2
            - è³ªå•ï¼š`Snowflakeã¨Redshiftã®ãƒ¡ãƒªãƒƒãƒˆã‚’æ•™ãˆã¦ä¸‹ã•ã„ã€‚`
            - è¿”å´å€¤ï¼š`['Snowflake', 'Redshift']`
            ###ä¾‹3
            - è³ªå•ï¼š`Reactã®stateã¨refã®é•ã„ã‚’æ•™ãˆã¦ãã ã•ã„`
            - è¿”å´å€¤ï¼š`['React','state','ref']`
            ###ä¾‹4
            - è³ªå•ï¼š`Pythonã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã§ãŠã™ã™ã‚ãªã‚‚ã®ã¯ï¼Ÿ`
            - è¿”å´å€¤ï¼š`['Python','ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†']`
            ###ä¾‹5
            - è³ªå•ï¼š`ã‚­ãƒƒãƒˆã‚«ãƒƒãƒˆã¨ã‚¬ãƒªã‚¬ãƒªå›ã ã£ãŸã‚‰ã©ã£ã¡ã‚’è²·ã†ã®ãŒã„ã„ã§ã™ã‹ï¼Ÿ`
            - è¿”å´å€¤ï¼š`['ã‚­ãƒƒãƒˆã‚«ãƒƒãƒˆ','ã‚¬ãƒªã‚¬ãƒªå›']`
        """
        st.session_state.messages = [
            # {"role": "system", "content": "You are a helpful assistant."}
            {"role": "system", "content": system_prompt}
        ]
        # st.session_state.messages = []
        st.session_state.costs = []


def converse_with_ai(model_name, temperature):
    container = st.container()
    with container:
        with st.form(key="your_form", clear_on_submit=True):
            user_input = st.text_area(
                label=st.session_state.lg_Can_I_ask_a_question,
                key="input",
                height=100,
                disabled=not st.session_state.llm_connection_flag,
            )

            # leave nothing against line breaks
            content = user_input.replace("\n", "")
            submit_button = st.form_submit_button(
                label=st.session_state.lg_Send,
                type="primary",
                disabled=not st.session_state.llm_connection_flag,
            )

    if submit_button and user_input:
        st.session_state.messages.append({"role": "user", "content": content})
        with st.spinner(f"{st.session_state.lg_Processing}"):
            messages = st.session_state.messages
            response = openai.chat.completions.create(
                model=model_name,
                messages=messages,  # æ–‡å­—åˆ—ã«å¤‰æ›ã—ãŸmessagesã‚’æ¸¡ã™
                temperature=temperature,
            )

        keyword_list = ast.literal_eval(response.choices[0].message.content)
        results = search_index(keyword_list)
        reference_list = []
        for result in results:
            name_and_page_list = [result["book_name"], f"{result['page_number']}ãƒšãƒ¼ã‚¸"]
            reference_list.append(name_and_page_list)
        reference_contents = "ä»¥ä¸‹ã®æ›¸ç±ã®ãƒšãƒ¼ã‚¸ãŒå‚è€ƒã«ãªã‚Šã¾ã™\n\n"
        for reference in reference_list:
            reference_contents += f"- æ›¸ç±åï¼š{reference[0]}\n- ãƒšãƒ¼ã‚¸ï¼š{reference[1]}\n---\n"
        st.session_state.messages.append(
            {"role": "assistant", "content": reference_contents}
        )

    # present chat history
    messages = st.session_state.messages
    for message in messages:
        if message["role"] == "assistant":
            # NOTE:Streamlit 1_22_0ã§ã¯ã€st.chat_messageã¯ä½¿ãˆãªã„
            with st.chat_message("assistant"):
                st.markdown(message["content"])
            # st.info(f"""ğŸ¤– Assistant: {message['content']}""")
        elif message["role"] == "user":
            with st.chat_message("user"):
                st.markdown(message["content"])
            # st.success(f"""ğŸ˜€ You: {message['content']}""")


if __name__ == "__main__":
    main()
